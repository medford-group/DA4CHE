
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Regression &#8212; Data Analytics for Chemical Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1-numerical_methods/Topic1.3-Linear_Regression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Numerical Optimization" href="Topic1.4-Numerical_Optimization.html" />
    <link rel="prev" title="Linear Algebra" href="Topic1.2-Linear_Algebra.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Analytics for Chemical Engineers</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Numerical Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Topic1.1-Python_Basics.html">Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="Topic1.2-Linear_Algebra.html">Linear Algebra</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="Topic1.4-Numerical_Optimization.html">Numerical Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2-regression/intro.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.1-Non-parametric_Models.html">Non-parametric Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.2-Model_Validation.html">Model Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.3-Complexity_Optimization.html">Complexity Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.4-High_Dimensional_Regression.html">High-dimensional Regression</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/1-numerical_methods/Topic1.3-Linear_Regression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression">Polynomial Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-linear-regression">General Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-in-scikit-learn">Linear Regression in Scikit-Learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-reading">Additional Reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <nav class="contents local" id="contents" role="doc-toc">
<ul class="simple">
<li><p><a class="reference internal" href="#linear-regression" id="id1">Linear Regression</a></p>
<ul>
<li><p><a class="reference internal" href="#learning-objectives" id="id2">Learning Objectives</a></p></li>
<li><p><a class="reference internal" href="#simple-linear-regression" id="id3">Simple linear regression</a></p></li>
<li><p><a class="reference internal" href="#polynomial-regression" id="id4">Polynomial Regression</a></p></li>
<li><p><a class="reference internal" href="#general-linear-regression" id="id5">General Linear Regression</a></p></li>
<li><p><a class="reference internal" href="#linear-regression-in-scikit-learn" id="id6">Linear Regression in Scikit-Learn</a></p></li>
<li><p><a class="reference internal" href="#additional-reading" id="id7">Additional Reading</a></p></li>
</ul>
</li>
</ul>
</nav>
<section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><a class="toc-backref" href="#id1" role="doc-backlink">Linear Regression</a><a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Learning Objectives</a><a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>After completing this lecture, students should be able to:</p>
<ul class="simple">
<li><p>Define simple, polynomial, and general linear regression models.</p></li>
<li><p>Derive the least squares solution using matrix calculus.</p></li>
<li><p>Construct basis function matrices (e.g., Vandermonde, Gaussian).</p></li>
<li><p>Fit models using both manual matrix operations and <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p></li>
<li><p>Interpret model outputs and assess fit quality visually and numerically.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>  <span class="c1"># standard and recommended import convention</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../settings/plot_style.mplstyle&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="simple-linear-regression">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Simple linear regression</a><a class="headerlink" href="#simple-linear-regression" title="Link to this heading">#</a></h2>
<p>Linear regression is a great starting point for understanding how linear algebra and optimization are used together for data analytics. We will start with simple linear regression, which you should be familiar with.</p>
<p>The form of a simple linear regression model is given as:</p>
<div class="math notranslate nohighlight">
\[
y = m x + b + \epsilon
\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the independent variable, <span class="math notranslate nohighlight">\(x\)</span> is the dependent variable, <span class="math notranslate nohighlight">\(m\)</span> is the slope of the line, <span class="math notranslate nohighlight">\(b\)</span> is the intercept, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error between the model and the actual data. This can also be written with indices on the data:</p>
<div class="math notranslate nohighlight">
\[
y_i = m x_i + b + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> refers to the index of the data point (e.g., the first, second, third, … data point). We can also think of these quantities as vectors:</p>
<div class="math notranslate nohighlight">
\[
\vec{y} = m\vec{x} + b + \vec{\epsilon}
\]</div>
<p>To make things consistent with prior lectures, we can re-write this as:</p>
<div class="math notranslate nohighlight">
\[
y_i = w_0 x_i^0 + w_1 x_i^1 + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_0 = b\)</span> and <span class="math notranslate nohighlight">\(w_1 = m\)</span>. Now we can re-write this as a matrix-vector product:</p>
<div class="math notranslate nohighlight">
\[
y_i = \sum_{j=0}^1 w_j x_i^{j} + \epsilon_i
\]</div>
<p>If you recall the Vandermonde matrix, this can be written as:</p>
<div class="math notranslate nohighlight">
\[
\vec{y} = \bar{\bar{X}}\vec{w} + \vec{\epsilon}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> is the first-order Vandermonde matrix. We can create a dataset that satisfies this model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># make x into a column vector</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;--o&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b0fbf8b5b1a9cffa38fcf5bb1cf5ed8252bf699b7b5728c6c5ff11c13b72403e.png" src="../_images/b0fbf8b5b1a9cffa38fcf5bb1cf5ed8252bf699b7b5728c6c5ff11c13b72403e.png" />
</div>
</div>
<p>We are still missing the <span class="math notranslate nohighlight">\(\epsilon\)</span> term. This is the error, and in linear regression we assume that the error follows a normal distribution. We can generate a vector of normally-distributed noise using NumPy and add it to <span class="math notranslate nohighlight">\(\vec{y}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">normal</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The error term is modeled as normally distributed noise with mean 0 because we assume the model captures the true underlying trend, and the remaining differences are unbiased random variation. This is a standard assumption in statistical modeling and helps ensure that parameter estimates are not systematically biased.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;--o&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">epsilon</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/28f56105c8e5856514c2a5e8e8671d7d1e14eaa7f07ee22dc01a64b9d928bf64.png" src="../_images/28f56105c8e5856514c2a5e8e8671d7d1e14eaa7f07ee22dc01a64b9d928bf64.png" />
</div>
</div>
<p>The goal of linear regression is to use the data, <span class="math notranslate nohighlight">\(y_i\)</span>, to recover the best-fit line. In this case, we know the answer since we generated the data. However, we can also try to recover the line based only on the noisy data.</p>
<p>There are multiple ways to derive linear regression, but here we will derive it by minimizing the sum of squared errors. This is the origin of the name “least squares”: we want to find the line that gives the lowest squared errors. First, we will set up a cost function that quantifies the squared errors:</p>
<div class="math notranslate nohighlight">
\[
g = \sum_j \epsilon_j^2
\]</div>
<p>Next, we can recall the definition of an inner product to see that <span class="math notranslate nohighlight">\(\sum_j \epsilon_j^2 = \vec{\epsilon}^T \vec{\epsilon}\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[
g = \vec{\epsilon}^T \vec{\epsilon}.
\]</div>
<p>Next, we can re-arrange our expression for the model to solve for <span class="math notranslate nohighlight">\(\vec{\epsilon}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\vec{y} = \bar{\bar{X}} \vec{w} + \vec{\epsilon}
\quad\Longrightarrow\quad
\vec{\epsilon} = \vec{y} - \bar{\bar{X}} \vec{w}
\]</div>
<p>Next we substitute this into the loss function</p>
<div class="math notranslate nohighlight">
\[
g = \vec{\epsilon}^T \vec{\epsilon}
  = (\vec{y} - \bar{\bar{X}} \vec{w})^T (\vec{y} - \bar{\bar{X}} \vec{w})
\]</div>
<p>Recalling matrix transpose rules:</p>
<div class="math notranslate nohighlight">
\[
(\vec{y} - \bar{\bar{X}} \vec{w})^T (\vec{y} - \bar{\bar{X}} \vec{w})
= (\vec{y}^T - \vec{w}^T \bar{\bar{X}}^T) (\vec{y} - \bar{\bar{X}} \vec{w})
\]</div>
<p>and multiplying:</p>
<div class="math notranslate nohighlight">
\[
\vec{w}^T \bar{\bar{X}}^T \bar{\bar{X}} \vec{w}
- \vec{y}^T \bar{\bar{X}} \vec{w}
- \vec{w}^T \bar{\bar{X}}^T \vec{y}
+ \vec{y}^T \vec{y}
\]</div>
<p>the middle two terms are both dot products of <span class="math notranslate nohighlight">\(\vec{y}^T(\bar{\bar{X}}\vec{w})\)</span> or <span class="math notranslate nohighlight">\((\bar{\bar{X}}\vec{w})^T \vec{y}\)</span>, which are transposes of each other, and scalar quantities. The transpose of a scalar is equal to the same scalar, so these terms are equal and can be combined giving:</p>
<div class="math notranslate nohighlight">
\[
g = \vec{w}^T \bar{\bar{X}}^T \bar{\bar{X}} \vec{w}
  - 2 \vec{y}^T \bar{\bar{X}} \vec{w}
  + \vec{y}^T \vec{y}
\]</div>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>What is <span class="math notranslate nohighlight">\(g\)</span> a function of? Although <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are commonly treated as variables in engineering, here they are fixed data inputs. The only true variable in the loss function <span class="math notranslate nohighlight">\(g(\vec{w})\)</span> is <span class="math notranslate nohighlight">\(\vec{w}\)</span> — the vector of unknown parameters we want to optimize. This distinction is important when taking derivatives: we differentiate with respect to <span class="math notranslate nohighlight">\(\vec{w}\)</span>, holding <span class="math notranslate nohighlight">\(\vec{y}\)</span> and <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> constant.</p>
</div>
<hr class="docutils" />
<p>We now have the sum of squared errors quantified as a function of the weights, <span class="math notranslate nohighlight">\(\vec{w}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
g(\vec{w})
= \vec{w}^T \bar{\bar{X}}^T \bar{\bar{X}} \vec{w}
- 2 \vec{y}^T \bar{\bar{X}} \vec{w}
+ \vec{y}^T \vec{y}
\]</div>
<p>Now we can recall the definition of minima from calculus: the derivative of a function at a minimum (or maximum) is 0. This implies that we need to take the derivative of the loss function with respect to the parameters <span class="math notranslate nohighlight">\(\vec{w}\)</span> and set it equal to zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial g}{\partial \vec{w}} = 0
\]</div>
<p>Taking derivatives with respect to vectors can be tricky, but the following two identities are useful:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial (\bar{\bar{A}} \vec{x})}{\partial \vec{x}} = \bar{\bar{A}}^T
\qquad
\frac{\partial (\vec{x}^T \bar{\bar{A}} \vec{x})}{\partial \vec{x}}
= (\bar{\bar{A}}^T + \bar{\bar{A}})\vec{x}
\]</div>
<p>Using these identities you should be able to show that:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial g}{\partial \vec{w}}
= 2 \bar{\bar{X}}^T \bar{\bar{X}} \vec{w}
- 2 \bar{\bar{X}}^T \vec{y}
\]</div>
<p>Setting equal to zero and re-arranging gives:</p>
<div class="math notranslate nohighlight">
\[
\bar{\bar{X}}^T \bar{\bar{X}} \vec{w} = \bar{\bar{X}}^T \vec{y}
\]</div>
<p>Now we can notice that <span class="math notranslate nohighlight">\(\bar{\bar{X}}^T \bar{\bar{X}}\)</span> is a matrix, which we can call <span class="math notranslate nohighlight">\(\bar{\bar{A}}\)</span>, and <span class="math notranslate nohighlight">\(\bar{\bar{X}}^T \vec{y}\)</span> is a vector, which we can call <span class="math notranslate nohighlight">\(\vec{b}\)</span>. If we let <span class="math notranslate nohighlight">\(\vec{w} = \vec{x}\)</span> then we can see that this is a system of linear equations:</p>
<div class="math notranslate nohighlight">
\[
\bar{\bar{A}} \vec{x} = \vec{b}
\]</div>
<hr class="docutils" />
<p>Let’s set this up in Python for our toy problem:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
<span class="n">w_lsr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights from least-squares regression: </span><span class="si">{</span><span class="n">w_lsr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original weights to generate data: </span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights from least-squares regression: [ 1.42744455 -2.50869849]
Original weights to generate data: [1.4, -2.5]
</pre></div>
</div>
</div>
</div>
<p>We see that the results are not identical but are close. We can also check the quality of the best-fit line visually:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w_lsr</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;ok&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span> <span class="o">@</span> <span class="n">w</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/4a62e700b422254bdd6d3fbee0cde1457fee9ae30f756486f5167885bb11f1fb.png" src="../_images/4a62e700b422254bdd6d3fbee0cde1457fee9ae30f756486f5167885bb11f1fb.png" />
</div>
</div>
<p>We can see that the best-fit line actually fits the data better than the original weights! We will explore more strategies for quantifying the fit in the “regression” lecture.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Modify the code so that the true slope and intercept used to generate the data can be changed. Try adjusting the slope, intercept, and noise level, and observe how this affects the quality of the fit. Can you find a situation where the regression fails to recover the original model?</p>
</div>
</section>
<section id="polynomial-regression">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Polynomial Regression</a><a class="headerlink" href="#polynomial-regression" title="Link to this heading">#</a></h2>
<p>If you have seen simple linear regression before, the derivation above probably seemed far more complex than what you have seen in the past. However, the advantage is that it is also much more general, as we will see when moving to polynomial regression.</p>
<p>In polynomial regression, we expand the model to be of the form:</p>
<div class="math notranslate nohighlight">
\[
y_i = w_0 + w_1 x_i + w_2 x_i^2 + w_3 x_i^3 + \dots + \epsilon_i
\]</div>
<p>As before, we can write this in summation notation:</p>
<div class="math notranslate nohighlight">
\[
y_i = \sum_{j=0}^m w_j x_i^{j} + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the order of the highest polynomial. Recalling the definition of the Vandermonde matrix, we see that this can also be written as:</p>
<div class="math notranslate nohighlight">
\[
\vec{y} = \bar{\bar{X}} \vec{w} + \vec{\epsilon}
\]</div>
<p>which is identical to the form that we used for linear regression. The only difference is that now the matrix <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> has <span class="math notranslate nohighlight">\(m+1\)</span> columns instead of 2 columns. This means that we can use the same solution from linear regression for polynomial regression!</p>
<div class="math notranslate nohighlight">
\[
\bar{\bar{X}}^T \bar{\bar{X}} \vec{w} = \bar{\bar{X}}^T \vec{y}
\]</div>
<p>Let’s see an example. First, we can generate some data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#make x into a column vector</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x</span><span class="o">**</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">))</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@w</span> <span class="o">+</span> <span class="n">epsilon</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;o&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c730013557286887de961913107c5de111cc70bb1dcb6f6ca5b10234016d3e5b.png" src="../_images/c730013557286887de961913107c5de111cc70bb1dcb6f6ca5b10234016d3e5b.png" />
</div>
</div>
<p>Now we can try to recover the weights, <span class="math notranslate nohighlight">\(w\)</span> using the same math as before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span>
<span class="n">w_lsr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Weights from least-squares regression: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_lsr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original weights to generate data: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights from least-squares regression: [ 1.13100455  0.91469945 -0.31120658  0.02033033]
Original weights to generate data: [1.4, 0.8, -0.3, 0.02]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@w_lsr</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span><span class="nd">@w</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/951ce4dd2f8972e5db3aefb0105809df2f25890c7a5b9799eaa3c53d60f868e5.png" src="../_images/951ce4dd2f8972e5db3aefb0105809df2f25890c7a5b9799eaa3c53d60f868e5.png" />
</div>
</div>
<p>In this case we cheated a bit, since we knew that the data was generated from a third-order polynomial. We can also find fits based on lower (or higher) orders of polynomials by modifying the order of the Vandermonde matrix used in the least-squares equation. First, we can make a function that creates a Vandermonde matrix of any order:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">vandermonde</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">):</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">order</span><span class="p">):</span>
        <span class="n">cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span>

<span class="n">X_vdm</span> <span class="o">=</span> <span class="n">vandermonde</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">X_vdm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[   1.,    0.,    0.,    0.],
       [   1.,    1.,    1.,    1.],
       [   1.,    2.,    4.,    8.],
       [   1.,    3.,    9.,   27.],
       [   1.,    4.,   16.,   64.],
       [   1.,    5.,   25.,  125.],
       [   1.,    6.,   36.,  216.],
       [   1.,    7.,   49.,  343.],
       [   1.,    8.,   64.,  512.],
       [   1.,    9.,   81.,  729.],
       [   1.,   10.,  100., 1000.]])
</pre></div>
</div>
</div>
</div>
<p>Now we can repeat the fitting procedure with different orders:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">order</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">X_vdm</span> <span class="o">=</span> <span class="n">vandermonde</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">X_vdm</span><span class="o">.</span><span class="n">T</span><span class="nd">@X_vdm</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">X_vdm</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span>
<span class="n">w_lsr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Weights: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_lsr</span><span class="p">))</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">X_vdm</span><span class="nd">@w_lsr</span>
<span class="n">SSE</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of Squared Errors (g): </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SSE</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">&#39;--*&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights: [ 1.21716365e+00  3.70723072e+00 -6.56580521e+00  4.85114381e+00
 -1.82140647e+00  3.76125525e-01 -4.34511188e-02  2.63503848e-03
 -6.53107619e-05]
Sum of Squared Errors (g): 0.02055183895251995
</pre></div>
</div>
<img alt="../_images/5cc52854ed65678d38994a4637b97ecd17bc49794b080c600e513b8d25c862de.png" src="../_images/5cc52854ed65678d38994a4637b97ecd17bc49794b080c600e513b8d25c862de.png" />
</div>
</div>
<p>We see that as the order of the polynomial increases, the sum of squared errors decreases. However, we are only checking the behavior of our model at the points we use to train it. We can also use the model to interpolate between points or extrapolate to new points by creating a new Vandermonde matrix with more rows. Increasing the resolution adds rows within the original range, and results in interpolation, while increasing the range will result in extrapolation. Let’s try both:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x_new</span> <span class="o">=</span> <span class="n">x_new</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1">#create column vector</span>
<span class="n">x_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_vdm_new</span> <span class="o">=</span> <span class="n">vandermonde</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">order</span><span class="p">)</span>
<span class="n">X_vdm_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100, 9)
</pre></div>
</div>
</div>
</div>
<p>We can create a new dataset based on this new higher resolution data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat_new</span> <span class="o">=</span> <span class="n">X_vdm_new</span><span class="nd">@w_lsr</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">yhat_new</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ce79b976badab134d06bf4040ef7df76f7cd508a5d6bce173f918fc1aab60a54.png" src="../_images/ce79b976badab134d06bf4040ef7df76f7cd508a5d6bce173f918fc1aab60a54.png" />
</div>
</div>
<p>We will discuss more techniques for validating the model and assessing the best order of the polynomial in future lectures.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Try setting <code class="docutils literal notranslate"><span class="pre">order</span> <span class="pre">=</span> <span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">order</span> <span class="pre">=</span> <span class="pre">3</span></code>, and <code class="docutils literal notranslate"><span class="pre">order</span> <span class="pre">=</span> <span class="pre">15</span></code> in the Vandermonde matrix. Use the same <code class="docutils literal notranslate"><span class="pre">x_new</span></code> vector to interpolate and extrapolate. Plot the predictions and observe how the fit changes inside vs. outside the training data range.</p>
</div>
</section>
<section id="general-linear-regression">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">General Linear Regression</a><a class="headerlink" href="#general-linear-regression" title="Link to this heading">#</a></h2>
<p>We just saw that a model of the form:</p>
<p><span class="math notranslate nohighlight">\(\vec{y} = \bar{\bar{X}}\vec{w} + \vec{\epsilon}\)</span></p>
<p>can be used for simple linear regression (if <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> is a first-order Vandermonde matrix) or polynomial regression (if <span class="math notranslate nohighlight">\(\bar{\bar{X}\)</span> is a higher-order Vandermonde matrix). In fact, this form can be used for many different types of linear regression and is referred to as a <strong>general linear model</strong>. Note that this is different from a <em>generalized linear model</em>, where the error term is assumed to follow a distribution other than normal. This is very confusing, but not terribly relevant in practice.</p>
<p>The key concept is that the columns of <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> can contain any type(s) of linearly-dependent non-linear functions and the math will remain the same:</p>
<p><span class="math notranslate nohighlight">\(\bar{\bar{X}}^T\bar{\bar{X}}\vec{w^*} = \bar{\bar{X}}^T\vec{y}\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\vec{w^*}\)</span> are the optimal least-squares parameters.</p>
<p>We will call the columns of <span class="math notranslate nohighlight">\(\bar{\bar{X}}\)</span> the “basis functions” for general linear regression. One common technique is the use of Gaussians as the basis functions. We can demonstrate this with a more realistic dataset. We will load in a dataset from infrared spectroscopy of an ethanol molecule.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/ethanol_IR.csv&#39;</span><span class="p">)</span>
<span class="n">x_all</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_all</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;absorbance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_all</span><span class="p">,</span> <span class="n">y_all</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/025efd4b752c309aafa0e6102905d83888a3a2fe6fa8353c6ce2898df7dac897.png" src="../_images/025efd4b752c309aafa0e6102905d83888a3a2fe6fa8353c6ce2898df7dac897.png" />
</div>
</div>
<p>This data looks a lot more complicated than our toy dataset from before. Let’s make things easier by just selecting one of the peaks:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_peak</span> <span class="o">=</span> <span class="n">x_all</span><span class="p">[</span><span class="mi">475</span><span class="p">:</span><span class="mi">575</span><span class="p">]</span>
<span class="n">y_peak</span> <span class="o">=</span> <span class="n">y_all</span><span class="p">[</span><span class="mi">475</span><span class="p">:</span><span class="mi">575</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0f9099a7811ee7d6b29e337f71bfd93bf5d9988af99d482f8f3e0d526d28e140.png" src="../_images/0f9099a7811ee7d6b29e337f71bfd93bf5d9988af99d482f8f3e0d526d28e140.png" />
</div>
</div>
<p>Let’s try fitting this with two manually placed Gaussians:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_peak</span> <span class="o">=</span> <span class="n">x_peak</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#convert x_peak into a vector</span>
<span class="n">X_gauss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x_peak</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">X_gauss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_peak</span> <span class="o">-</span> <span class="mi">2900</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">25</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">X_gauss</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_peak</span> <span class="o">-</span> <span class="mi">2980</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">25</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">X_gauss</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">X_gauss</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/aa824b64ac0b3626eed19d6842b078426e51d3b0276e0364319edee4eabc9015.png" src="../_images/aa824b64ac0b3626eed19d6842b078426e51d3b0276e0364319edee4eabc9015.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">X_gauss</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_gauss</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">X_gauss</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_peak</span>
<span class="n">w_lsr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">X_gauss</span> <span class="o">@</span> <span class="n">w_lsr</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Weights from least-squares regression: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_lsr</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights from least-squares regression: [0.54548962 0.67533912]
</pre></div>
</div>
<img alt="../_images/5f98df6379016323313415df34d394f6463637d0ffb96e702d11d7c911583099.png" src="../_images/5f98df6379016323313415df34d394f6463637d0ffb96e702d11d7c911583099.png" />
</div>
</div>
<p>This looks much better than the polynomial fit, and we only needed 2 parameters! But we still had to guess the peak positions and widths.</p>
<p>Let’s automate this using a more general method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">gaussian_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xk_vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">N</span><span class="p">)</span>
    <span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">xk</span> <span class="ow">in</span> <span class="n">xk_vec</span><span class="p">:</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">xk</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>Use this to fit the spectrum:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">X_gauss</span> <span class="o">=</span> <span class="n">gaussian_features</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="c1">#generate features</span>
<span class="n">A_m</span> <span class="o">=</span> <span class="n">X_gauss</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_gauss</span>
<span class="n">b_m</span> <span class="o">=</span> <span class="n">X_gauss</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y_peak</span>
<span class="n">w_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">A_m</span><span class="p">,</span> <span class="n">b_m</span><span class="p">)</span>

<span class="n">yhat_m</span> <span class="o">=</span> <span class="n">X_gauss</span> <span class="o">@</span> <span class="n">w_m</span>
<span class="n">SSE_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_peak</span> <span class="o">-</span> <span class="n">yhat_m</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Weights: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">w_m</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sum of Squared Errors: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">SSE_m</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">yhat_m</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights: [ 0.00618991  0.01283186  0.02895143  0.02964597  0.08421234  0.42542746
  0.18814892  0.574715    0.02757571 -0.00945447  0.00639036 -0.00400029]
Sum of Squared Errors: 0.007590570362196521
</pre></div>
</div>
<img alt="../_images/87a8002158bbfdf46cd79bdaf94889bfd67f8a7c872a8857752d6209a0393687.png" src="../_images/87a8002158bbfdf46cd79bdaf94889bfd67f8a7c872a8857752d6209a0393687.png" />
</div>
</div>
<p>This model now uses 12 Gaussian basis functions spaced evenly across the selected region of the spectrum. Each basis function captures a portion of the absorbance profile, and the regression coefficients determine how much each feature contributes to the total fit.</p>
<p>By solving the least squares problem for this set of features, we are able to approximate the shape of the spectrum quite accurately. The peaks are reconstructed not by guessing their locations, but by allowing the model to find an optimal combination of these fixed-width features.</p>
<p>The number of features (<code class="docutils literal notranslate"><span class="pre">m</span></code>) and the width of the Gaussian peaks (<code class="docutils literal notranslate"><span class="pre">sigma</span></code>) both influence how flexible the model is. If <code class="docutils literal notranslate"><span class="pre">sigma</span></code> is too large, peaks may blur together; if it is too small, the fit may become noisy and sensitive to small variations in the data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This modeling approach resembles traditional spectral peak fitting but should be interpreted differently. In typical spectroscopy, positive peak heights reflect physical quantities. Here, negative weights are allowed and may appear, not because of physical meaning, but because the model optimizes a numerical error. The goal is best fit, not physical interpretability.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Try modifying the <code class="docutils literal notranslate"><span class="pre">sigma</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">gaussian_features</span></code> function. How does increasing or decreasing the width affect the fit? Try values of 10, 25, and 50, and observe which features are emphasized or smoothed out.</p>
</div>
</section>
<section id="linear-regression-in-scikit-learn">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Linear Regression in Scikit-Learn</a><a class="headerlink" href="#linear-regression-in-scikit-learn" title="Link to this heading">#</a></h2>
<p>So far we have solved our general linear regression models directly from linear algebra. This is relatively easy, but it still requires us to set up a linear system and solve it. There is a very useful Python package called <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> that has implementations of many commonly-used algorithms, including general linear models.</p>
<p>We will introduce <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> here to show how it can make things simpler. One thing to note is that <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> only solves the regression part of the problem, so we still need to set up the feature matrix, <span class="math notranslate nohighlight">\(X_{ij}\)</span>. We will keep working with the spectra example, and use the <code class="docutils literal notranslate"><span class="pre">gaussian_features</span></code> function that we wrote earlier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">X_m</span> <span class="o">=</span> <span class="n">gaussian_features</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span> <span class="c1">#generate features</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span> <span class="c1">#create a linear regression model instance</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_m</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">)</span> <span class="c1">#fit the model (equivalent to the linear solve)</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_m</span><span class="p">)</span> <span class="c1">#create the model prediction (equivalent to the matrix multiplication)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/086ebf2087e9e5f518f5f88a7c70ef913ba62814c16b4f26249f942f44fbe9f3.png" src="../_images/086ebf2087e9e5f518f5f88a7c70ef913ba62814c16b4f26249f942f44fbe9f3.png" />
</div>
</div>
<p>We can see that this requires much less code. However, <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is heavily “object” or “class” based, which can make the syntax confusing if you aren’t familiar with Python. The advantage is that it is very easy to change the model and compare performance, as we will see in future lectures.</p>
<p>The key takeaway is that <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> abstracts away the manual matrix operations and provides a streamlined interface. This can save time and reduce coding errors, especially when using more complex models. However, it also means that we give up a bit of transparency — it’s important to understand what’s happening under the hood.</p>
<p>By using <code class="docutils literal notranslate"><span class="pre">fit()</span></code> and <code class="docutils literal notranslate"><span class="pre">predict()</span></code>, we are replicating the behavior of solving <span class="math notranslate nohighlight">\(\bar{X}^T\bar{X}\vec{w} = \bar{X}^T\vec{y}\)</span> and then evaluating <span class="math notranslate nohighlight">\(\bar{X}\vec{w}\)</span>. The weights themselves can be accessed via <code class="docutils literal notranslate"><span class="pre">model.coef_</span></code>, and the intercept via <code class="docutils literal notranslate"><span class="pre">model.intercept_</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Learned weights:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Learned weights: [-0.02758914 -0.01444074 -0.00059751  0.00110594  0.05520992  0.39660691
  0.15932838  0.54571257 -0.00096432 -0.0390034  -0.02088223 -0.03777934]
Intercept: 0.04286995631699525
</pre></div>
</div>
</div>
</div>
<p>This is useful if you want to inspect the result or transfer the model elsewhere.</p>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Try increasing the number of Gaussian features <code class="docutils literal notranslate"><span class="pre">m</span></code> from 12 to 20 and observe how the fit changes. Does it improve? What happens if you reduce it to 5? Use <code class="docutils literal notranslate"><span class="pre">model.coef_</span></code> to inspect how many of the features have nonzero weight.</p>
</div>
</section>
<section id="additional-reading">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Additional Reading</a><a class="headerlink" href="#additional-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">scikit-learn: Linear regression documentation</a></p></li>
<li><p><a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statistical Learning – Hastie, Tibshirani, Friedman</a></p></li>
<li><p>[Pattern Recognition and Machine Learning – Bishop, Chapter 3]</p></li>
<li><p><a class="reference external" href="https://cs229.stanford.edu/notes2022fall/cs229-notes1.pdf">CS229 Lecture Notes on Linear Regression – Stanford</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./1-numerical_methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Topic1.2-Linear_Algebra.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Linear Algebra</p>
      </div>
    </a>
    <a class="right-next"
       href="Topic1.4-Numerical_Optimization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Numerical Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-regression">Polynomial Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-linear-regression">General Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-in-scikit-learn">Linear Regression in Scikit-Learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-reading">Additional Reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By A.J. Medford
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  © 2025 A.J. Medford
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>