{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7c9f6d",
   "metadata": {},
   "source": [
    "# High-dimensional Regression\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Learning objectives\n",
    "- Explain the “curse of dimensionality” and why high-dimensional feature spaces complicate model fitting and generalization.\n",
    "- Visualize multivariate feature relationships and identify collinearity using plots and summary statistics.\n",
    "- Standardize and transform features and targets appropriately; justify when scaling is required.\n",
    "- Construct and evaluate multiple linear regression models in high-dimensional settings with proper validation.\n",
    "- Apply dimensionality reduction (e.g., PCA) and interpret explained variance and loadings; perform principal component regression and compare to baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../settings/plot_style.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389fdd1",
   "metadata": {},
   "source": [
    "## High-dimensional Data\n",
    "\n",
    "So far we have only worked with datasets that have a single input dimension. We have generated \"features\" from this dimension, but we have not considered the case of a problem where multiple inputs are given. This is a very common scenario, and one of the main advantages of many machine-learning methods is that they work well for \"high-dimensional\" data, or data with many features.\n",
    "\n",
    "In this lecture we will work with a dataset of chemical process data provided by Dow Chemical. The data comes from a generic chemical process with the following setup:\n",
    "\n",
    "<img src=\"images/dow_process.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4da5b6",
   "metadata": {},
   "source": [
    "The dataset contains a number of operating conditions for each of the units in the process, as well as the concentration of impurities in the output stream. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "df.head(10) # shows the first 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb7c81",
   "metadata": {},
   "source": [
    "In order to work with this data, we need to \"clean\" it to remove missing values. We will come back to this in the \"data management\" module. For now, just run the cell below and it will create a matrix `X` of inputs and `y` of impurity concentrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f551fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust check for real, finite numeric values\n",
    "# (returns False for strings, None, NaN, inf, and complex numbers)\n",
    "def is_real_and_finite(x):\n",
    "    try:\n",
    "        val = float(x)\n",
    "    except (TypeError, ValueError):\n",
    "        return False\n",
    "    return np.isfinite(val)\n",
    "\n",
    "all_data = df[df.columns[1:]].values  # drop the first column (date)\n",
    "numeric_map = df[df.columns[1:]].applymap(is_real_and_finite)\n",
    "real_rows = numeric_map.all(axis=1).copy().values  # True if all values in a row are real numbers\n",
    "X = np.array(all_data[real_rows, :-5], dtype=float)  # drop the last 5 cols that are not inputs\n",
    "y = np.array(all_data[real_rows, -3], dtype=float)\n",
    "y = y.reshape(-1, 1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889317a",
   "metadata": {},
   "source": [
    "This is the dataset we will work with. We have 10297 data points, with 40 input variables (features) and one output variable. We can pull the names of the features (and output) in case we forget later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = [str(x) for x in df.columns[1:41]]\n",
    "y_name = str(df.columns[-3])\n",
    "print(y_name)\n",
    "x_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d060a3",
   "metadata": {},
   "source": [
    "Don't worry if all this code doesn't make sense, we will revisit `pandas` in more detail later. The goal is to predict the output, impurity, as a function of all the input variables.\n",
    "\n",
    "```{admonition} Exercise: Printing out the names of features\n",
    ":class: tip\n",
    "\n",
    "Write a short script to print a \"key\" for the feature names, with the index and name of each feature. You may find the \"enumerate\" function very useful for this.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b26eb53",
   "metadata": {},
   "source": [
    "## Visualization of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f86e24",
   "metadata": {},
   "source": [
    "Unlike working with a single variable where we can plot \"x vs. y,\" it is harder to build intuition for **higher-dimensional** data because we cannot directly visualize all dimensions at once. A good first step is to look at histograms of each input variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0980800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X dimensions: {X.shape}')\n",
    "print(f'Feature names: {x_names}')\n",
    "\n",
    "N = X.shape[-1]\n",
    "n = int(np.sqrt(N))\n",
    "fig, axes = plt.subplots(n, n + 1, figsize=(6*n, 6*n))\n",
    "ax_list = axes.ravel()\n",
    "\n",
    "for i in range(N):\n",
    "    ax_list[i].hist(X[:, i], bins=30, alpha=0.85)\n",
    "    ax_list[i].set_xlabel(x_names[i])\n",
    "\n",
    "# Hide any unused axes (if grid has extra panels)\n",
    "for j in range(N, len(ax_list)):\n",
    "    ax_list[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3956f4",
   "metadata": {},
   "source": [
    "We can see that some features are approximately normally distributed, while others have obvious outliers or bimodal shapes.\n",
    "\n",
    "```{note}\n",
    "**Why might there be bimodal distributions in a chemical process?**  \n",
    "Chemical processes often operate in distinct modes. For example, equipment may be \"on\" vs. \"off,\" or a plant may switch among steady-state setpoints (e.g., different product grades, feedstocks, or throughput targets). Such regime changes naturally yield bimodal (or multimodal) feature distributions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d49f8",
   "metadata": {},
   "source": [
    "### Visualizing two features at a time and other first-pass strategies\n",
    "\n",
    "A simple next step after histograms is to examine **bivariate** relationships:\n",
    "\n",
    "- **Scatter plots** for selected feature pairs.\n",
    "- **Color by the target** (`y`) to see how the response varies in the plane.\n",
    "- **Small multiples** (pairwise grid) for a *subset* of features when `N` is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick two features to compare\n",
    "i, j = 0, 1  # change indices to explore other pairs\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sc = plt.scatter(X[:, i], X[:, j], c=y.ravel(), s=10)\n",
    "plt.xlabel(x_names[i]); plt.ylabel(x_names[j])\n",
    "plt.title(f'{x_names[i]} vs {x_names[j]} (colored by {y_name})')\n",
    "cbar = plt.colorbar(sc); cbar.set_label(y_name)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small pair grid for a handful of features\n",
    "subset_idx = [0, 1, 2, 3]  # choose a small set to keep plots readable\n",
    "m = len(subset_idx)\n",
    "fig, axes = plt.subplots(m, m, figsize=(3.2*m, 3.2*m))\n",
    "\n",
    "for r, ii in enumerate(subset_idx):\n",
    "    for c, jj in enumerate(subset_idx):\n",
    "        ax = axes[r, c]\n",
    "        if r == c:\n",
    "            ax.hist(X[:, ii], bins=30, alpha=0.85)\n",
    "            ax.set_ylabel('count')\n",
    "        else:\n",
    "            ax.scatter(X[:, jj], X[:, ii], s=6, alpha=0.6)\n",
    "        if r == m-1: ax.set_xlabel(x_names[jj])\n",
    "        if c == 0:   ax.set_ylabel(x_names[ii])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43bd62",
   "metadata": {},
   "source": [
    "We can also look for feature relationships through the **covariance matrix**. The covariance describes how features vary together. We will not go through the math here, but we will discuss the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = np.cov(X.T)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "c = ax.imshow(covar)\n",
    "ax.set_title('Feature Covariance Matrix')\n",
    "fig.colorbar(c);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a742c",
   "metadata": {},
   "source": [
    "This matrix suggests that some features are highly correlated. We can inspect specific entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal entries are variances (depend on scale); off-diagonals are covariances.\n",
    "print(f'Variance of {x_names[2]}: {covar[2,2]:.3g}')\n",
    "print(f'Variance of {x_names[1]}: {covar[1,1]:.3g}')\n",
    "# Uncomment to inspect an off-diagonal covariance:\n",
    "# print(f'Covariance({x_names[2]}, {x_names[3]}): {covar[2,3]:.3g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f17e7f",
   "metadata": {},
   "source": [
    "These numbers are difficult to compare across features because **covariance depends on units/scale** (e.g., °C vs. bar). A scale-invariant alternative is the **correlation** matrix, which rescales by standard deviations and lies in [-1, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X.T)\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "im = ax.imshow(corr, vmin=-1, vmax=1)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "fig.colorbar(im, ax=ax, label='Pearson r')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363fc85",
   "metadata": {},
   "source": [
    "```{admonition} Exercise: Highly correlated bivariate scatter plots\n",
    ":class: tip\n",
    "\n",
    "\n",
    "1. Programmatically find the top 5 **absolute** correlations among **distinct** feature pairs (ignore the diagonal).\n",
    "2. Make bivariate scatter plots for those 5 pairs (use small markers and `alpha=0.5`). Color each scatter by `y` to see whether the strongest feature–feature correlations also correspond to structure in the target.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Scaling Features and Outputs\n",
    "\n",
    "We can see that different features have very different ranges, and different units (e.g., degrees, percent, count). Scaling data is like \"non-dimensionalizing\" or normalizing for different units. This is often critical to ensure that certain variables are not weighted more than others.\n",
    "\n",
    "Statistical methods do not know about physical units, so we can normalize or \"scale\" features to aid in comparison:\n",
    "\n",
    "- rescaling: 0 = min, 1 = max\n",
    "- mean scaling: 0 = mean, 1 = max, -1 = min\n",
    "- **standard scaling: 0 = mean, 1 = standard deviation**\n",
    "- unit vector: the length of each multi-dimensional vector is 1\n",
    "\n",
    "See the [scikit-learn documentation](http://scikit-learn.org/stable/modules/preprocessing.html) for more examples and discussion.\n",
    "\n",
    "Note that scaling is not always a good idea. Sometimes the data have units that are already consistent, or rescaling can remove important aspects of the data. Figuring out the best scaling scheme is often achieved through trial and error.\n",
    "\n",
    "```{note}\n",
    "The term \"scaling\" is common throughout science and engineering and has many different meanings. For example, in chemical engineering we often talk about \"scaling\" a process up, and in physics quantities are sometimes said to \"scale\" if they are proportional, leading to \"scaling relationships\". Even within computer science, \"scaling\" means different things -- the way an algorithm \"scales\" refers to how much time it takes as the size of the problem or the size of the computer changes. The term \"feature scaling\" is less ambiguous, and can also be referred to as \"feature normalization\".\n",
    "```\n",
    "\n",
    "It is also important to note that feature scaling is a common source of **data leakage**. Data leakage occurs when information from the testing set \"leaks\" into the training data, which can lead to artificially good results when the model is applied to the test set. It might seem like feature scaling is not really a \"model\", but in reality you are using the data to determine the parameters (mean, min, max, or standard deviation) for the scaling. It is important that you always perform any data splitting for cross validation **before** performing feature scaling, and you should only use the training data to determine the scaling parameters. When applying the feature scaling to the testing data, you will still use the parameters (e.g., mean, min, max, or standard deviation) from the training data. Think of an application scenario where you train a model to a large amount of data, but then want to apply it to a single new data point: you would not be able to calculate the mean, min, max, or standard deviation of that data point, so you would need to use the statistics of the training data instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cfc70",
   "metadata": {},
   "source": [
    "In the case of the Dow chemical process data, we can look at the features and see they clearly have different units and different ranges. For example, feature 1 (Primary column tails flow) ranges from 0 to 50, and feature 2 (Input to primary column Bed 3 Flow) ranges from 0 to \\~3000. While we do not necessarily know the units (since this is proprietary data), we can see that there is a difference of range. This is why the covariance matrix did not make much sense. We can rescale the data to put everything on similar scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0525f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X) # \"fit\" the scaler (finds the mean and standard deviation of the data)\n",
    "X_scaled = ss.transform(X) #\"transform\" the data by applying the standard scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8612b",
   "metadata": {},
   "source": [
    "The `fit` and `transform` methods may seem unintuitive or unnecessarily complex, but this standard interface to `scikit-learn` models makes it possible to \"chain\" them together and use them interchangeably. We will see an example of this shortly as we create a \"pipeline\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d928250",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**What could go wrong with rescaling or mean scaling?**  \n",
    "Min–max and mean scaling can be **highly sensitive to outliers**. A single extreme value can dominate the min/max (or mean) and compress most of the other data into a narrow interval, making patterns hard to see and potentially hurting model performance. Robust alternatives include scaling with quantiles or medians/MAD.\n",
    "```\n",
    "\n",
    "Now let's take a look at the covariance matrix again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum entry in convariance\n",
    "print(f\"max entry in covariance matrix: {covar.max():.3f}\")\n",
    "\n",
    "# Maximum off-diagonal entry of the covariance/correlation matrix\n",
    "N = covar.shape[0]\n",
    "tri = np.triu_indices(N, k=1)  # upper triangle, exclude diagonal\n",
    "vals = np.abs(covar[tri])\n",
    "idx = np.argmax(vals)\n",
    "i_max, j_max = tri[0][idx], tri[1][idx]\n",
    "print(f\"max off-diagonal covariance: {vals[idx]:.3f} at ({x_names[i_max]}, {x_names[j_max]}) [indices: {i_max}, {j_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b39ab",
   "metadata": {},
   "source": [
    "We see that the rank is 40, but the maximum covariance is 1. The reason is that the diagonal entries of the standardized covariance (i.e., correlation) matrix will always be 1 since features are perfectly correlated with themselves. We see that the maximum off-diagonal is less than one, so the sanity check passes.\n",
    "\n",
    "In general, if the data have been **standard scaled** (with the same ddof convention), then the covariance matrix will range from -1 to 1 and is equivalent to a correlation matrix, which can also be computed directly from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960fa9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.corrcoef(X.T)\n",
    "covar = np.cov(X_scaled.T)\n",
    "np.isclose(corr, covar, atol=1e-4).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: leakage-safe scaling with scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "pipe = make_pipeline(StandardScaler(), LinearRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f\"Test R^2 (pipeline with scaling): {pipe.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e03f7f",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**Use `make_pipeline` for clean and repeatable preprocessing**  \n",
    "The `make_pipeline` function chains preprocessing and modeling steps into a single estimator that fits and predicts in the correct order. For example,\n",
    "`pipe = make_pipeline(StandardScaler(with_mean=True, with_std=True), LinearRegression())`\n",
    "creates a workflow that:\n",
    "\n",
    "- **Fits scaling only on training data** when you call `pipe.fit(X_train, y_train)`, preventing leakage.\n",
    "- **Applies the same scaling to new/test data** automatically inside `pipe.predict(X_test)`.\n",
    "- **Plays nicely with cross-validation** (`cross_val_score`, `GridSearchCV`): the scaler is refit **inside each fold**. When tuning hyperparameters of a downstream model, use the step name with a double underscore, e.g. `{'ridge__alpha': [0.1, 1, 10]}` for `make_pipeline(StandardScaler(), Ridge())`.\n",
    "- Lets you **access the final model** with `pipe[-1]` or `pipe.named_steps['linearregression']` to inspect coefficients, etc.\n",
    "\n",
    "This pattern is the standard way to keep preprocessing and modeling coupled, avoid data leakage, and ensure reproducible evaluation. You can also use the approach to chain together different types of models or functions to create complex pipelines that act as a single estimator. \n",
    "```\n",
    "\n",
    "We will discuss the covariance/correlation matrix more later, but when dealing with multi-dimensional data it is always good to check."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ac96a",
   "metadata": {},
   "source": [
    "```{admonition} Exercise: Apply and explore scaling approaches\n",
    ":class: tip\n",
    "\n",
    "1. Split `X, y` into train/test (80/20) with `random_state=0`.\n",
    "2. Fit a `StandardScaler` **only on the training set**, then transform both train and test.\n",
    "3. Confirm that the **training** features have mean≈0 and std≈1 (per feature). Report the maximum absolute deviation from 0 (means) and 1 (stds).\n",
    "4. Explore at least one other scaling strategy on the data and visualize the re-scaled features with both approaches using histograms.\n",
    "```\n",
    "\n",
    "\n",
    "## Multi-Linear Regression\n",
    "\n",
    "We can recall the general form of a linear regression model:\n",
    "\n",
    "\\$y\\_i = \\sum\\_j w\\_j X\\_{ij} + \\epsilon\\_i\\$\n",
    "\n",
    "Previously, we created features (columns of \\$X\\$) by transforming the original 1-dimensional input. In this case, we already have columns of \\$X\\$ provided from the data. We can use the same linear regression techniques from before. To keep preprocessing safe and tidy, we will use a **pipeline** that standard-scales features before linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Build and fit a leakage-safe pipeline on the full dataset (illustrative only)\n",
    "pipe = make_pipeline(StandardScaler(with_mean=True, with_std=True), LinearRegression())\n",
    "pipe.fit(X, y)\n",
    "r2 = pipe.score(X, y)\n",
    "\n",
    "yhat = pipe.predict(X)\n",
    "print(f\"r^2 = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e070f",
   "metadata": {},
   "source": [
    "We see that the \\$r^2\\$ score is 0.71, which is not terrible, but not great either. We cannot really visualize the model since we have 40-dimensional inputs. However, we can make a **parity plot**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "y_true = y.ravel()\n",
    "y_pred = yhat.ravel()\n",
    "ax.scatter(y_true, y_pred, alpha=0.15)\n",
    "\n",
    "# 45-degree reference line\n",
    "lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "ax.plot(lims, lims, '-k')\n",
    "ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "\n",
    "ax.set_xlabel('Actual Data')\n",
    "ax.set_ylabel('Predicted Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d78bcc",
   "metadata": {},
   "source": [
    "This looks reasonable, although there are quite a few outliers. We should also remember that we trained on all the data, so this might be over-fit. We can quickly test using **hold-out** cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c17ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Pipeline ensures scaling is fit only on the training data\n",
    "pipe = make_pipeline(StandardScaler(with_mean=True, with_std=True), LinearRegression())\n",
    "pipe.fit(X_train, y_train)\n",
    "r2_train = pipe.score(X_train, y_train)\n",
    "\n",
    "yhat_test = pipe.predict(X_test)\n",
    "r2_test = pipe.score(X_test, y_test)\n",
    "\n",
    "print(f\"r^2 train = {r2_train:.3f}\")\n",
    "print(f\"r^2 test  = {r2_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22532a",
   "metadata": {},
   "source": [
    "We see that they are comparable, which indicates that we have not over-fit. We can also visualize both training and testing errors with a parity plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "yhat_train = pipe.predict(X_train).ravel()\n",
    "yhat_test  = yhat_test.ravel()\n",
    "ax.scatter(y_train.ravel(), yhat_train, alpha=0.10, label='Training Set')\n",
    "ax.scatter(y_test.ravel(),  yhat_test,  alpha=0.10, label='Test Set')\n",
    "\n",
    "# 45-degree reference line common to both\n",
    "all_true = np.concatenate([y_train.ravel(), y_test.ravel()])\n",
    "all_pred = np.concatenate([yhat_train, yhat_test])\n",
    "lims = [min(all_true.min(), all_pred.min()), max(all_true.max(), all_pred.max())]\n",
    "ax.plot(lims, lims, '-k')\n",
    "ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "\n",
    "ax.set_xlabel('Actual Data')\n",
    "ax.set_ylabel('Predicted Data')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21676863",
   "metadata": {},
   "source": [
    "We can see that these look comparable, which confirms that we have not over-fit the model. It is always a good idea to check the parity plot to see if any patterns stand out!\n",
    "\n",
    "This basic linear regression model is simple, but by testing it we now have a **baseline model**. This tells us that if we have any results worse than this we have a really bad model!\n",
    "\n",
    "```{note}\n",
    "**What is a “baseline model,” and why use one?**  \n",
    "A baseline is the **simplest reasonable model** you can implement quickly and evaluate fairly. It establishes a **reference performance** so you can tell whether more complex methods add real value. A good baseline is:\n",
    "\n",
    "- **Simple/fast** and easy to explain (e.g., `make_pipeline(StandardScaler(), LinearRegression())`).\n",
    "- **Evaluated fairly** with proper splitting/CV, the same metrics, and fixed randomness.\n",
    "- **Reproducible** with recorded settings and code.\n",
    "\n",
    "If a new approach cannot **beat the baseline on held‑out data**, revisit your data, features, or evaluation before adding complexity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e70e6f",
   "metadata": {},
   "source": [
    "We see that the performance of the model is not great, and to improve things we will need to add some non-linearity. In 1-dimensional space we achieved this by adding transforms of the features as new features. However, this is more challenging in a high-dimensional space since the number of features will scale with the number of dimensions.\n",
    "\n",
    "```{note}\n",
    "**How many features would result if third-order interactions were considered?**  \n",
    "It depends on what you include:\n",
    "\n",
    "- If you mean **unique three-way interactions with distinct features only** (e.g., $x_i x_j x_k$ with $i<j<k$), the count is $\\binom{40}{3} = 9{,}880$.\n",
    "- If you include **all degree-3 polynomial terms with replacement** (e.g., $x_i^3,\\; x_i^2 x_j,\\; x_i x_j x_k$), the count is $\\binom{40 + 3 - 1}{3} = \\binom{42}{3} = 11{,}480$.\n",
    "\n",
    "Both grow **combinatorially** and illustrate why naive feature expansion becomes impractical in high dimensions.\n",
    "```\n",
    "\n",
    "Kernel-based methods are very commonly used for high-dimensional spaces because they account for non-linear interactions, but the number of features does not exceed the number of data points. In your homework you will explore the application of KRR to this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfef06",
   "metadata": {},
   "source": [
    "```{admonition}\n",
    ":class: tip\n",
    "\n",
    "Compare linear regression on the **original** features and on **rescaled** features.\n",
    "\n",
    "1. Split the raw data with `train_test_split(X, y, test_size=0.3, random_state=0)`.\n",
    "2. Fit `LinearRegression()` on the **raw** training features; record train/test $r^2$.\n",
    "3. Fit a pipeline `make_pipeline(StandardScaler(), LinearRegression())` on the **same** split; record train/test $r^2$.\n",
    "4. Show that the **predictions are (nearly) identical** on the test set by printing the max absolute difference between the two prediction vectors and the absolute difference in test $r^2$.\n",
    "5. Compare the **coefficients**: print the top-10 by absolute value for (a) the raw model and (b) the scaled model. *(Optional)* Recover coefficients on the **raw scale** from the pipeline (using the scaler’s `mean_` and `scale_`) and verify they match the raw model’s coefficients.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb397907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be260ae",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "An alternative approach to creating high-dimensional models is to reduce the dimensionality. We will briefly look at some techniques here, and revisit this idea later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d30c7a",
   "metadata": {},
   "source": [
    "### Forward Selection\n",
    "\n",
    "A very intuitive way to reduce dimensions is to just select a subset of the original features. The simplest strategy to select or rank features is to try them one-by-one, and keep the best feature at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "N_features = 40\n",
    "X_subset = X_scaled.copy()\n",
    "x_names_subset = np.array(x_names, dtype=object).copy()\n",
    "new_X = []\n",
    "new_X_names = []\n",
    "\n",
    "while len(new_X) < N_features and X_subset.shape[1] > 0:\n",
    "    r2_list = []\n",
    "    for j in range(X_subset.shape[1]):\n",
    "        model = LinearRegression()  # create a linear regression model instance\n",
    "        xj = X_subset[:, j].reshape(-1, 1)\n",
    "        model.fit(xj, y)            # fit the model\n",
    "        r2 = model.score(xj, y)     # r^2 for this single feature\n",
    "        r2_list.append((r2, j))\n",
    "    # select highest r^2 value\n",
    "    r2_list.sort(key=lambda t: t[0])\n",
    "    r2_max, j_max = r2_list[-1]\n",
    "    new_X.append(X_subset[:, j_max].copy())\n",
    "    new_X_names.append(x_names_subset[j_max])\n",
    "    # remove selected feature from the pool\n",
    "    x_names_subset = np.delete(x_names_subset, j_max)\n",
    "    X_subset = np.delete(X_subset, j_max, axis=1)\n",
    "\n",
    "print('The {} most linearly correlated features are:'.format(len(new_X)))\n",
    "print(new_X_names)\n",
    "\n",
    "new_X = np.array(new_X).T  # shape: (n_samples, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3720652",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**About this \"forward selection\" loop**  \n",
    "This implementation performs **univariate ranking** (pick best single feature, then the next best single feature, etc.). A full **forward stepwise** method would re-fit a multi-feature model at each step using the features already chosen **plus** each candidate feature, selecting the one that improves the model the most. We use the simpler ranking here for speed and clarity.\n",
    "```\n",
    "\n",
    "We can see how the \\$r^2\\$ score changes with the reduced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e24bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()  # create a linear regression model instance\n",
    "model.fit(new_X, y)         # fit the model on the selected features\n",
    "r2 = model.score(new_X, y)  # r^2 on the same data\n",
    "print(\"r^2 = {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d672d8",
   "metadata": {},
   "source": [
    "We see that with just 4 features the model performance is substantially reduced. We can keep increasing the number until it is comparable to the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef4fab",
   "metadata": {},
   "source": [
    "```{admonition}\n",
    ":class: tip\n",
    "\n",
    "Below we compute $r^2$ as we include more ranked features and identify the **minimum** number of features needed to reach $r^2\\ge 0.60$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "k_max = new_X.shape[1]\n",
    "r2_path = []\n",
    "ks = np.arange(1, k_max + 1)\n",
    "\n",
    "for k in ks:\n",
    "    Xk = new_X[:, :k]model = LinearRegression().fit(Xk, y)\n",
    "    r2_path.append(model.score(Xk, y))\n",
    "\n",
    "r2_path = np.array(r2_path)\n",
    "# first k reaching target (if any)\n",
    "target = 0.60\n",
    "hit = np.argmax(r2_path >= target) if np.any(r2_path >= target) else None\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ks, r2_path, 'o-')\n",
    "plt.axhline(target, linestyle='--')\n",
    "if hit is not None and r2_path[hit] >= target:\n",
    "    plt.axvline(ks[hit], linestyle=':')\n",
    "plt.xlabel('Number of selected features (k)')\n",
    "plt.ylabel('$r^2$ (fit on selected features)')\n",
    "plt.title('Forward selection (univariate ranking)')\n",
    "plt.tight_layout()\n",
    "\n",
    "if hit is not None and r2_path[hit] >= target:\n",
    "    print(f\"Minimum k achieving r^2 ≥ {target}: {ks[hit]} (r^2 = {r2_path[hit]:.3f})\")\n",
    "else:\n",
    "    print(f\"Target r^2 ≥ {target} not reached with up to {k_max} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b28fb0",
   "metadata": {},
   "source": [
    "Be careful, since just because features are not *linearly* correlated does not mean that they are not *non-linearly* correlated (in other words, we might reject a feature that is actually very descriptive, if that description is highlight non-linear) . There is also no guarantee that we are not finding correlated features, since if one feature has a high correlation with the output, and is also correlated with another feature, then that feature will also be correlated with the output. More advanced forward selection strategies can be used to reduce this, as shown with a standard implementation below:\n",
    "\n",
    "**Standard scikit-learn (full forward stepwise) feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full forward stepwise selection using scikit-learn's SequentialFeatureSelector (SFS)\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Use a pipeline so scaling is learned inside each CV fold (avoids leakage)\n",
    "est = make_pipeline(StandardScaler(with_mean=True, with_std=True), LinearRegression())\n",
    "\n",
    "# Choose how many features to keep (example: 10) and use forward stepwise with cross-validation\n",
    "sfs = SequentialFeatureSelector(\n",
    "    est, n_features_to_select=10, direction=\"forward\",\n",
    "    scoring=\"r2\", cv=5, n_jobs=-1\n",
    ").fit(X, y.ravel())\n",
    "\n",
    "selected_idx = sfs.get_support(indices=True)\n",
    "selected_names = [x_names[i] for i in selected_idx]\n",
    "\n",
    "print(\"Selected feature indices:\", selected_idx)\n",
    "print(\"Selected feature names:\", selected_names)\n",
    "\n",
    "# Transform X to the selected subset and fit a final model on the whole dataset\n",
    "X_fs = sfs.transform(X)\n",
    "final_model = LinearRegression().fit(X_fs, y)\n",
    "print(\"r^2 on full data using selected subset:\", final_model.score(X_fs, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d71d2",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**Choosing `n_features_to_select`**  \n",
    "`SequentialFeatureSelector` requires you to specify how many features to keep. In practice you can sweep over `k` (e.g., 1–20) and pick the smallest `k` that reaches a target cross-validated score, or use a validation curve to balance performance and parsimony.\n",
    "```\n",
    "\n",
    "### Principal component analysis\n",
    "\n",
    "An alternative strategy to avoid having correlated features is to ensure that features are orthogonal using the eigenvectors of the covariance matrix. The code below finds the eigenvectors of the covariance matrix, which we know will be orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the (standardized) covariance matrix for PCA\n",
    "cov = np.cov(X_scaled.T)\n",
    "\n",
    "# For symmetric matrices (covariance), use eigh (guaranteed real eigenvalues)\n",
    "vals, vecs = np.linalg.eigh(cov)  # vals ascending\n",
    "# sort descending by variance explained\n",
    "idx = np.argsort(vals)[::-1]\n",
    "PCvals = vals[idx]\n",
    "PCvecs = vecs[:, idx]\n",
    "\n",
    "# sanity checks: orthonormal eigenvectors\n",
    "print('dot(PC1, PC1) ~', float(np.dot(PCvecs[:, 0], PCvecs[:, 0])))\n",
    "print('dot(PC1, PC2) ~', float(np.dot(PCvecs[:, 0], PCvecs[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45b692",
   "metadata": {},
   "source": [
    "It turns out that by taking the eigenvalues of the covariance matrix you are actually doing something called \"principal component analysis\", which is a classic dimensionality reduction technique. The eigenvectors of the covariance matrix identify the \"natural\" coordinate system of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D example to visualize a dominant principal direction\n",
    "x = np.random.normal(0, 3, 150)\n",
    "y = x - 15 + 2 - 3 * np.random.random(len(x))\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(x, y, '.', alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('PCA toy example');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a439135",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**PCA coordinates vs. Cartesian coordinates**  \n",
    "Think of the usual x–y axes as a fixed Cartesian frame. PCA rotates this frame to a new set of perpendicular axes (the principal components) that align with the directions of greatest variance in the data—similar to choosing an origin and x- and y-axis when solving an engineering problem. The new axes are orthonormal (like the unit vectors i, j, k), and projecting data onto them is just taking dot products with these unit vectors. In this rotated frame, covariances vanish (the off‑diagonals go to ~0), so variability is concentrated along a few axes, making analysis and modeling simpler, but the underlying data is not really changed.\n",
    "```\n",
    "\n",
    "The eigenvalues provide the variance in each direction, and we can use this to determine how much variance each principal component contributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4986ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_variance = np.sum(PCvals)\n",
    "explained_variance = PCvals / total_variance  # already sorted desc\n",
    "\n",
    "print('Total variance (trace of covariance):', float(total_variance))\n",
    "print('First 5 explained variance ratios:', np.round(explained_variance[:5], 4))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(explained_variance, 'o', label='Variance')\n",
    "ax.plot(np.cumsum(explained_variance), 'o', label='Cumulative variance')\n",
    "ax.axhline(0.9, linestyle='--', color='k')\n",
    "ax.set_xlabel('PCA #th Dimension')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.legend(loc='best');\n",
    "\n",
    "# Report how many components capture 90% variance\n",
    "k90 = np.searchsorted(np.cumsum(explained_variance), 0.9) + 1\n",
    "print(f\"Components needed for ≥90% variance: {k90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f56396",
   "metadata": {},
   "source": [
    "We can use this to say how many principal components are needed to capture a specified fraction of the variance (e.g., 90%).\n",
    "\n",
    "Finally, we can \"project\" the data onto the principal components. This is equivalent to re-defining the axes of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_projection = np.dot(X_scaled, PCvecs)\n",
    "print('Projection shape:', PC_projection.shape)\n",
    "\n",
    "corr_PCs = np.corrcoef(PC_projection.T)\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "c = ax.imshow(corr_PCs, vmin=-1, vmax=1)\n",
    "fig.colorbar(c);\n",
    "ax.set_title('Correlation among PCs (should be ~identity)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8baa63",
   "metadata": {},
   "source": [
    "After projection, we still have 40 features but they are now orthogonal - there is no covariance! This means that each one contains unique information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b06c6",
   "metadata": {},
   "source": [
    "We will talk a lot more about PCA throughout the course, but for now you should know:\n",
    "\n",
    "- Principal component vectors are obtained from the eigenvectors of the covariance matrix\n",
    "- Principal components are orthogonal\n",
    "- Principal components explain the variance in multi-dimensional data\n",
    "- Data can be projected onto principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c43290",
   "metadata": {},
   "source": [
    "```{admonition}\n",
    ":class: tip\n",
    "\n",
    "Using `sklearn.decomposition.PCA`, verify that scikit-learn's results match the manual PCA above.\n",
    "\n",
    "1. Fit `PCA()` on `X_scaled`. Inspect the key attributes: `n_components_`, `components_` (principal axes), `explained_variance_`, `explained_variance_ratio_`, `singular_values_`, and `mean_`.\n",
    "2. Compare `explained_variance_` with the eigenvalues you computed above (`PCvals`). They should match (within numerical tolerance).\n",
    "3. Compare principal axes: `components_.T` should equal `PCvecs` **up to sign**. (Hint: compute `R = PCvecs.T @ components_.T` and use `np.sign(np.diag(R))` to align signs.)\n",
    "4. Compare scores/projections: `pca.transform(X_scaled)` should match your `PC_projection` **up to the same signs**.\n",
    "5. Plot the explained variance ratio and its cumulative sum; confirm it matches the manual curves.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Principal Component Regression\n",
    "\n",
    "We can also use the projected data as inputs to a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad900664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Use the PCA projection computed earlier in this topic (PC_projection)\n",
    "model = LinearRegression()  # create a linear regression model instance\n",
    "model.fit(PC_projection, y)  # fit the model\n",
    "r2 = model.score(PC_projection, y)  # r^2 on the same data\n",
    "print(f\"r^2 = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa529398",
   "metadata": {},
   "source": [
    "Let's compare this to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ac883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()  # create a linear regression model instance\n",
    "model.fit(X_scaled, y)      # fit the model on scaled original features\n",
    "r2 = model.score(X_scaled, y)\n",
    "print(f\"r^2 = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6e382",
   "metadata": {},
   "source": [
    "We see that the answer is the same. This is because we are still ultimately including all the same information. However, if we want to reduce the number of features we will see a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "\n",
    "model_PC = LinearRegression()\n",
    "model_PC.fit(PC_projection[:, :N], y)\n",
    "r2 = model_PC.score(PC_projection[:, :N], y)\n",
    "print(f\"r^2 PCA = {r2:.3f}\")\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled[:, :N], y)\n",
    "r2 = model.score(X_scaled[:, :N], y)\n",
    "print(f\"r^2 regular = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b6aa3",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**Why is a PCR model not always better than direct linear regression?**  \n",
    "PCA orders directions by **variance in X**, not by how well they **predict y**. A high‑variance component can be weakly related (or unrelated) to the target, while a lower‑variance component might carry most of the predictive signal. PCR is unsupervised in its dimensionality reduction; it ignores `y` when choosing components.\n",
    "```\n",
    "\n",
    "The PCA projection collects as much information as possible in each feature and orders components by variance. We can also check them one‑by‑one to see how they correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for j in range(PC_projection.shape[1]):\n",
    "    model = LinearRegression()\n",
    "    xj = PC_projection[:, j].reshape(-1, 1)\n",
    "    model.fit(xj, y)\n",
    "    r2 = model.score(xj, y)\n",
    "    score_list.append((r2, j))\n",
    "\n",
    "score_list.sort(reverse=True)\n",
    "for r2j, j in score_list:\n",
    "    print(f\"PC{j:02d} : r^2 = {r2j:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46dca1",
   "metadata": {},
   "source": [
    "We see that the second principal component is actually the best, the first is the second best, and the seventh is third best. This is because the principal components only use variance of the inputs, which may or may not correlate to the outputs.&#x20;\n",
    "\n",
    "It is common to use PCA or other dimensionality reduction techniques prior to regression when working with high-dimensional data. It is often possible to construct models that have better performance with fewer input dimensions.\n",
    "\n",
    "### Partial Least Squares (PLS)\n",
    "\n",
    "Unlike PCA—which is **unsupervised** and finds directions of maximum variance in **X**—Partial Least Squares is **supervised**: it finds latent components that maximize the **covariance between X and y**. As a result, PLS components are chosen to be predictive of the target. PLS is especially helpful when there are many collinear features and relatively few samples. We will return to PLS and supervised dimensionality reduction later in the course, but it is useful to contrast it with principal component regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29356120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLS via scikit-learn (with scaling and a held-out test split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Choose number of latent components (tune this via CV in practice)\n",
    "pls_k = 6\n",
    "pipe_pls = make_pipeline(StandardScaler(), PLSRegression(n_components=pls_k))\n",
    "pipe_pls.fit(X_train, y_train.ravel())\n",
    "print(f\"PLS (k={pls_k}) train r^2: {pipe_pls.score(X_train, y_train):.3f}\")\n",
    "print(f\"PLS (k={pls_k})  test r^2: {pipe_pls.score(X_test,  y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ad342",
   "metadata": {},
   "source": [
    "```{note}\n",
    "**Tuning PLS components**  \n",
    "Select `n_components` with cross‑validation. With a pipeline, you can use `GridSearchCV` and the step name `plsregression__n_components`, e.g. `{'plsregression__n_components': range(1, min(20, X.shape[1]) + 1)}`. \n",
    "```\n",
    "\n",
    "### A note on neural networks\n",
    "Neural networks (including “deep learning”) are among the best-known examples of *high-dimensional regression* models. We do not cover them here because they introduce additional concepts (e.g., network architectures, activation functions, backpropagation/optimizers, and regularization) and are challenging to use well in practice due to many hyperparameters and training options. Entire courses are dedicated to neural networks. If this topic interests you, consider exploring a dedicated deep-learning resource (e.g. the textbook *Deep Learning* by Goodfellow, Bengio, and Courville) after you are comfortable with the basic concepts and ideas developed in this course.\n",
    "\n",
    "\n",
    "```{admonition} Exercise: Making a PCA pipeline\n",
    ":class: tip\n",
    "\n",
    "1. Split the data with `train_test_split(X, y, test_size=0.3, random_state=0)`.\n",
    "2. Build two pipelines:\n",
    "   - `pipe_lr = make_pipeline(StandardScaler(), LinearRegression())`\n",
    "   - `pipe_pcr_k = make_pipeline(StandardScaler(), PCA(n_components=k), LinearRegression())`\n",
    "solute difference in $r^2$).\n",
    "3. Show that when `k` is equal to the total number of features, the results are the same. \n",
    "```\n",
    "\n",
    "\n",
    "(sec:2.4-additional-reading)=\n",
    "## Additional reading\n",
    "\n",
    "- (hastie-esl-ch3)= Hastie, Tibshirani, & Friedman (2009). *The Elements of Statistical Learning*, 2nd ed., Ch. 3–4 (Linear Methods; Basis Expansions).\n",
    "- (islr-ch6-7)= James, Witten, Hastie, & Tibshirani (2013). *An Introduction to Statistical Learning*, Ch. 6–7 (Linear Model Selection; Moving Beyond Linearity).\n",
    "- (bishop-pca)= Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*, Ch. 12 (Dimensionality Reduction).\n",
    "- (wold-pls)= Wold, S., Sjöström, M., & Eriksson, L. (2001). “PLS-regression: a basic tool of chemometrics.” *Chemometrics and Intelligent Laboratory Systems*, **58**(2), 109–130.\n",
    "- (sklearn-pca)= Scikit-learn User Guide: Decomposition — Principal Component Analysis.\n",
    "- (sklearn-pls)= Scikit-learn User Guide: Partial Least Squares Regression."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
