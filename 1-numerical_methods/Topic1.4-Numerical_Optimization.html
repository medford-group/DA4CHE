
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Numerical Optimization &#8212; Data Analytics for Chemical Engineers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '1-numerical_methods/Topic1.4-Numerical_Optimization';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Regression" href="../2-regression/intro.html" />
    <link rel="prev" title="Linear Regression" href="Topic1.3-Linear_Regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Analytics for Chemical Engineers</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Numerical Methods</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Topic1.1-Python_Basics.html">Python Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="Topic1.2-Linear_Algebra.html">Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="Topic1.3-Linear_Regression.html">Linear Regression</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Numerical Optimization</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../2-regression/intro.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.1-Non-parametric_Models.html">Non-parametric Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.2-Model_Validation.html">Model Validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.3-Complexity_Optimization.html">Complexity Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2-regression/Topic2.4-High_Dimensional_Regression.html">High-dimensional Regression</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/1-numerical_methods/Topic1.4-Numerical_Optimization.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Numerical Optimization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">Non-linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criteria">Stopping Criteria</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-scipy">Optimization with Scipy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-reading">Additional Reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <nav class="contents local" id="contents" role="doc-toc">
<ul class="simple">
<li><p><a class="reference internal" href="#numerical-optimization" id="id1">Numerical Optimization</a></p>
<ul>
<li><p><a class="reference internal" href="#learning-objectives" id="id2">Learning Objectives</a></p></li>
<li><p><a class="reference internal" href="#non-linear-regression" id="id3">Non-linear Regression</a></p></li>
<li><p><a class="reference internal" href="#automatic-differentiation" id="id4">Automatic Differentiation</a></p></li>
<li><p><a class="reference internal" href="#gradient-descent" id="id5">Gradient Descent</a></p></li>
<li><p><a class="reference internal" href="#optimization-with-scipy" id="id6">Optimization with Scipy</a></p></li>
<li><p><a class="reference internal" href="#additional-reading" id="id7">Additional Reading</a></p></li>
</ul>
</li>
</ul>
</nav>
<section class="tex2jax_ignore mathjax_ignore" id="numerical-optimization">
<h1><a class="toc-backref" href="#id1" role="doc-backlink">Numerical Optimization</a><a class="headerlink" href="#numerical-optimization" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Learning Objectives</a><a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this lecture, you should be able to:</p>
<ul class="simple">
<li><p>Describe the structure of non-linear regression problems and formulate appropriate loss functions.</p></li>
<li><p>Use automatic differentiation to compute gradients of complex functions.</p></li>
<li><p>Implement gradient descent with different stopping criteria and interpret convergence behavior.</p></li>
<li><p>Apply numerical optimization tools (e.g., <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code>) to fit models to data.</p></li>
<li><p>Understand soft vs. hard constraints and when to use them in optimization problems.</p></li>
</ul>
<p>In this lecture we will continue to work with the ethanol peaks dataset and look at numerical optimization from the perspective of non-linear regression.</p>
<p>First, we can re-load the dataset and select the same region we were working on before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;../settings/plot_style.mplstyle&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/ethanol_IR.csv&#39;</span><span class="p">)</span>
<span class="n">x_all</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_all</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;absorbance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">x_peak</span> <span class="o">=</span> <span class="n">x_all</span><span class="p">[</span><span class="mi">475</span><span class="p">:</span><span class="mi">575</span><span class="p">]</span>
<span class="n">y_peak</span> <span class="o">=</span> <span class="n">y_all</span><span class="p">[</span><span class="mi">475</span><span class="p">:</span><span class="mi">575</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span><span class="n">y_peak</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;wavenumber [cm^-1]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;absorbance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0f9099a7811ee7d6b29e337f71bfd93bf5d9988af99d482f8f3e0d526d28e140.png" src="../_images/0f9099a7811ee7d6b29e337f71bfd93bf5d9988af99d482f8f3e0d526d28e140.png" />
</div>
</div>
</section>
<section id="non-linear-regression">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Non-linear Regression</a><a class="headerlink" href="#non-linear-regression" title="Link to this heading">#</a></h2>
<p>In the prior lecture we considered “general linear models” that followed the form:</p>
<p><span class="math notranslate nohighlight">\(y_i = \sum_j w_j X_{ij} + \epsilon_i\)</span></p>
<p>and all non-linear behavior has been captured by using non-linear transforms of <span class="math notranslate nohighlight">\(x_i\)</span>. However, in some cases we may want to optimize models that are not linear. For example, consider the Gaussian peak problem:</p>
<p><span class="math notranslate nohighlight">\(y_i = w_0 \exp\left(-\frac{(x_i-\mu_0)^2}{2(\sigma_0^2)}\right) + w_1 \exp\left(-\frac{(x_i-\mu_1)^2}{2(\sigma_1^2)}\right) + \epsilon_i\)</span></p>
<p>Previously we just guessed values for <span class="math notranslate nohighlight">\(\mu_i\)</span> and <span class="math notranslate nohighlight">\(\sigma_i\)</span>. However, it would be better if we could determine them from the data. Let’s go back to the derivation of the linear regression equations. Remember that our goal is to minimize the sum of squared errors:</p>
<p><span class="math notranslate nohighlight">\(g = \sum_i \epsilon_i^2\)</span></p>
<p>We can solve for <span class="math notranslate nohighlight">\(\epsilon_i\)</span> from the model:</p>
<p><span class="math notranslate nohighlight">\(\epsilon_i = y_i - w_0 \exp\left(-\frac{(x_i-\mu_0)^2}{2(\sigma_0^2)}\right) - w_1 \exp\left(-\frac{(x_i-\mu_1)^2}{2(\sigma_1^2)}\right) = y_i - \sum_j w_j G(x_i, \mu_j, \sigma_j)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(G(x_i, \mu_j, \sigma_j) = \exp\left(-\frac{(x_i-\mu_j)^2}{2(\sigma_j^2)}\right)\)</span>.</p>
<p>and substitute:</p>
<p><span class="math notranslate nohighlight">\(g = \sum_i (y_i - \sum_j w_j G(x_i, \mu_j, \sigma_j))^2\)</span></p>
<p>Now our loss function depends on all the parameters, <span class="math notranslate nohighlight">\(w_j\)</span>, <span class="math notranslate nohighlight">\(\mu_j\)</span>, and <span class="math notranslate nohighlight">\(\sigma_j\)</span>!</p>
<p><span class="math notranslate nohighlight">\(g(w_j, \mu_j, \sigma_j) = \sum_i (y_i - \sum_j w_j G(x_i, \mu_j, \sigma_j))^2\)</span></p>
<p>Let’s introduce a new vector, <span class="math notranslate nohighlight">\(\vec{\lambda}\)</span>, that is a vector containing all the parameters:</p>
<p><span class="math notranslate nohighlight">\(\vec{\lambda} = [\vec{w},\; \vec{\mu},\; \vec{\sigma}]\)</span></p>
<p>We can do this since <span class="math notranslate nohighlight">\(\lambda_{i\leq m}\)</span> contains the weights, <span class="math notranslate nohighlight">\(\lambda_{m &lt; i \leq 2m}\)</span> contains the means, and <span class="math notranslate nohighlight">\(\lambda_{i &gt; 2m}\)</span> contains the standard deviations. This is convenient since we can now write:</p>
<p><span class="math notranslate nohighlight">\(g(\lambda_j) = \sum_{i=0}^m (y_i - \sum_j \lambda_j G(x_i, \lambda_{m+j}, \lambda_{2m+j}))^2\)</span></p>
<p>and we can minimize the loss by setting the derivative equal to zero:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j} = 0\)</span></p>
<p>This may look scary, but we are actually just using multivariate calculus exactly like we did for linear regression. However, we are stuck with two new problems:</p>
<p>(1) Getting the derivative <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j}\)</span> will be very complicated.</p>
<p>(2) We need a way to find the point where <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j}=0\)</span>.</p>
<p>In the case of linear regression, we derived this with matrix algebra then solved the resulting equations but that will be much more difficult in this case, and will not be general to other non-linear forms. Instead, we will use numerical methods this time around.</p>
<p>First, we need to implement our loss function, <span class="math notranslate nohighlight">\(g\)</span>, which we will call <code class="docutils literal notranslate"><span class="pre">gaussian_loss</span></code> since it results from a sum of <span class="math notranslate nohighlight">\(m\)</span> Gaussians:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gaussian_loss</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="n">w_i</span> <span class="o">=</span> <span class="n">lamda</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">mu_i</span> <span class="o">=</span> <span class="n">lamda</span><span class="p">[</span><span class="n">m</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>
        <span class="n">sigma_i</span> <span class="o">=</span> <span class="n">lamda</span><span class="p">[</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">+</span> <span class="n">w_i</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu_i</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_i</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">squared_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">squared_error</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s do a sanity check by generating some data and testing the loss function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span> <span class="c1">#create a Gaussian with w=0.3, mu=0.2, sigma=0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.7</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mf">0.1</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#add a Gaussian with w=0.7, mu=0.5, sigma=0.1</span>
<span class="n">lamda</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span> <span class="c1">#create a &quot;lamda&quot; vector that should result in the same dataset</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">gaussian_loss</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">test_loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>np.float64(0.0)
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Try changing the number of peaks (<code class="docutils literal notranslate"><span class="pre">m=2</span></code>) in the <code class="docutils literal notranslate"><span class="pre">gaussian_loss</span></code> function to <code class="docutils literal notranslate"><span class="pre">m=3</span></code>, and create synthetic data with 3 Gaussians. Can your loss function still recover the correct loss value with the right parameters?</p>
</div>
</section>
<section id="automatic-differentiation">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Automatic Differentiation</a><a class="headerlink" href="#automatic-differentiation" title="Link to this heading">#</a></h2>
<p>Derivatives are needed a lot in machine learning. One development that has emerged from the fields of optimization and computer science is the idea of <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>, also sometimes called “algorithmic differentiation”. This is crucial to the success of well-known machine learning packages like “TensorFlow”. The details of how it works are far too advanced for this course, and we will not use it often. However, it is definitely worth knowing about since many engineering applications also require derivatives.</p>
<p>The simple version is that automatic differentiation does exactly what it sounds like: it gives you the derivative of a function automatically! We do need to use some special tools to do this in Python. The <code class="docutils literal notranslate"><span class="pre">autograd</span></code> package is the simplest, since it works well with <code class="docutils literal notranslate"><span class="pre">numpy</span></code>. We also need to write our functions in a specific way so that they only take one argument.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>autograd<span class="w"> </span>#&lt;-<span class="w"> </span>use<span class="w"> </span>this<span class="w"> </span>block<span class="w"> </span><span class="o">(</span>or<span class="w"> </span>the<span class="w"> </span><span class="nb">command</span><span class="w"> </span>after<span class="w"> </span>!<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>conda<span class="w"> </span>prompt<span class="o">)</span><span class="w"> </span>to<span class="w"> </span>install<span class="w"> </span>autograd
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: autograd in /Users/aj/opt/anaconda3/envs/DA4CHE/lib/python3.10/site-packages (1.8.0)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: numpy&lt;3 in /Users/aj/opt/anaconda3/envs/DA4CHE/lib/python3.10/site-packages (from autograd) (2.2.6)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">autograd.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>   <span class="c1"># autograd has its own &quot;version&quot; of numpy that must be used</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">autograd</span><span class="w"> </span><span class="kn">import</span> <span class="n">grad</span> <span class="c1"># the &quot;grad&quot; function provides derivatives</span>

<span class="k">def</span><span class="w"> </span><span class="nf">g</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gaussian_loss</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="n">diff_g</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">lamda</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diff_g</span><span class="p">(</span><span class="n">lamda</span><span class="p">))</span>
<span class="n">diff_g</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
[array(0.), array(0.), array(0.), array(0.), array(0.), array(0.)]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function autograd.wrap_util.unary_to_nary.&lt;locals&gt;.nary_operator.&lt;locals&gt;.nary_f(*args, **kwargs)&gt;
</pre></div>
</div>
</div>
</div>
<p>If you are not familiar with Python, this may look very odd. Essentially we are “wrapping” the <code class="docutils literal notranslate"><span class="pre">gaussian_loss</span></code> function in a new function <code class="docutils literal notranslate"><span class="pre">g</span></code>. Unlike <code class="docutils literal notranslate"><span class="pre">gaussian_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">g</span></code> only takes a single argument, <code class="docutils literal notranslate"><span class="pre">lamda</span></code>, which is the argument we want to differentiate with respect to.</p>
<p>It is also worth noting that the <code class="docutils literal notranslate"><span class="pre">grad</span></code> function returns a function, not a value. This will also feel odd if you are new to Python. However, it is very convenient, because now we can use the new <em>function</em> <code class="docutils literal notranslate"><span class="pre">diff_g</span></code> to compute the derivative at any arbitrary value of <span class="math notranslate nohighlight">\(\vec{\lambda}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">diff_g</span><span class="p">(</span><span class="n">lamda</span><span class="p">))</span>
<span class="n">lamda</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[array(0.), array(0.), array(0.), array(0.), array(0.), array(0.)]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.3, 0.7, 0.2, 0.5, 0.5, 0.1]
</pre></div>
</div>
</div>
</div>
<p>This is another sanity check: we know that the derivative should be zero if we are already at the optimum!</p>
<p>Let’s try with some other guess for <span class="math notranslate nohighlight">\(\lambda_j\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bad_guess</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">bad_guess</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diff_g</span><span class="p">(</span><span class="n">bad_guess</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.09629971881545715
[array(0.04543042), array(0.32856694), array(-0.01728235), array(-0.09807934), array(0.08818821), array(0.39993077)]
</pre></div>
</div>
</div>
</div>
<p>Now we have solved the first problem: we know how to get <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j}(\vec{\lambda})\)</span>. However, we do not have an analytical form (i.e. we can’t write it down), so we still don’t know how to solve for <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j} = 0\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When using Gaussian functions for regression or as basis functions, it’s important to remember that the gradient of a Gaussian decays rapidly as the input moves away from the mean. This can lead to <strong>vanishing gradients</strong>, especially if the input values are large (e.g., wavenumbers in the 1000–4000 cm⁻¹ range).</p>
<p>In practical terms, this means that for parameters corresponding to Gaussians far from the data, the gradient may be close to zero — even when the model is poor — because the loss function is nearly flat in that region. This can make optimization very difficult unless the parameters are initialized near reasonable values. It’s one of the reasons we often need good initial guesses or parameter constraints when fitting spectra with Gaussians.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vanish_guess</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">200.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span> <span class="c1">#make peaks very far away</span>
<span class="nb">print</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">vanish_guess</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">diff_g</span><span class="p">(</span><span class="n">vanish_guess</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.12008232596390747
[array(0.), array(0.), array(0.), array(0.), array(0.), array(0.)]
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Also note that to enable automatic differentiation, all the inputs need to be “floats”, not “integers”. Try removing the decimal from 100 and 200 above and see what happens.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Modify the <code class="docutils literal notranslate"><span class="pre">g</span></code> function so that it only includes <strong>one</strong> Gaussian (<code class="docutils literal notranslate"><span class="pre">m=1</span></code>). Generate synthetic data with a single peak centered at <code class="docutils literal notranslate"><span class="pre">μ</span> <span class="pre">=</span> <span class="pre">0.3</span></code>, with <code class="docutils literal notranslate"><span class="pre">σ</span> <span class="pre">=</span> <span class="pre">0.05</span></code>.</p>
<p>Now define two guesses for <code class="docutils literal notranslate"><span class="pre">lamda</span></code>:</p>
<ul class="simple">
<li><p>A <strong>good guess</strong> with parameters near the true values.</p></li>
<li><p>A <strong>bad guess</strong> where the Gaussian is centered far from the data (e.g. <code class="docutils literal notranslate"><span class="pre">μ</span> <span class="pre">=</span> <span class="pre">2.0</span></code>).</p></li>
</ul>
<p>Compute and compare the values of <code class="docutils literal notranslate"><span class="pre">g(lamda)</span></code> and <code class="docutils literal notranslate"><span class="pre">diff_g(lamda)</span></code> for both.</p>
<p><strong>What do you observe?</strong> This illustrates the <strong>vanishing gradient problem</strong>: when the Gaussian is too far from the data, the loss function becomes very flat and the gradient approaches zero, even if the model is very poor.</p>
</div>
</section>
<section id="gradient-descent">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Gradient Descent</a><a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>There are many numerical techniques for solving the problem of finding <span class="math notranslate nohighlight">\(\vec{\lambda}^*\)</span> such that <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial \lambda_j}(\vec{\lambda}^*) = 0\)</span>. The two basic approaches, which should be familiar, are:</p>
<ul class="simple">
<li><p>Newton’s method: Treat this as a root finding problem and use the second derivative, <span class="math notranslate nohighlight">\(\frac{\partial^2 g}{\partial \lambda_j \partial \lambda_k}\)</span> to iteratively optimize.</p></li>
<li><p>Gradient descent/ascent: Increase or decrease the guess by “walking” along the gradient.</p></li>
</ul>
<p>These are typically “iterative” methods, which means we start with some initial guess then iteratively improve it.</p>
<p>The simplest approach is to use gradient descent with a fixed step size, which we will explore here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">better_guess</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.21</span><span class="p">,</span> <span class="mf">0.52</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.11</span><span class="p">]</span>
<span class="n">guess</span> <span class="o">=</span> <span class="n">bad_guess</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Initial Loss: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">guess</span><span class="p">)))</span>

<span class="n">N_iter</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_iter</span><span class="p">):</span>
    <span class="n">guess</span> <span class="o">=</span> <span class="n">guess</span> <span class="o">-</span> <span class="n">h</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">diff_g</span><span class="p">(</span><span class="n">guess</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Final Loss: </span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">guess</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initial Loss: 0.0963
Final Loss: 0.0000
</pre></div>
</div>
</div>
</div>
<p>We can see that the loss decreases after 1000 iterations of gradient descent. Let’s compare the results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original Parameters: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">lamda</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Recovered Parameters: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">guess</span><span class="p">)))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_gaussians</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">w_0</span><span class="p">,</span> <span class="n">w_1</span><span class="p">,</span> <span class="n">mu_0</span><span class="p">,</span> <span class="n">mu_1</span><span class="p">,</span> <span class="n">sigma_0</span><span class="p">,</span> <span class="n">sigma_1</span> <span class="o">=</span> <span class="n">lamda</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">w_0</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">((</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">sigma_0</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span> <span class="o">+</span> <span class="n">w_1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu_1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma_1</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original Parameters: [0.3, 0.7, 0.2, 0.5, 0.5, 0.1]
Recovered Parameters: [0.69639616 0.30146582 0.49997652 0.20386904 0.09978107 0.49952471]
</pre></div>
</div>
<img alt="../_images/500b038ee02448693028cdecbdc6e6bbd4e68504b897d94f1af60eeb736a271f.png" src="../_images/500b038ee02448693028cdecbdc6e6bbd4e68504b897d94f1af60eeb736a271f.png" />
</div>
</div>
<p>We see that this looks pretty good! The parameters look different, but it turns out that they are pretty close if you switch the order of the two peaks.</p>
<section id="stopping-criteria">
<h3>Stopping Criteria<a class="headerlink" href="#stopping-criteria" title="Link to this heading">#</a></h3>
<p>In real applications, we often use a <strong>stopping criterion</strong> to decide when to terminate an optimization loop. Common stopping conditions include:</p>
<ul>
<li><p><strong>Fixed number of iterations</strong>: simple and guarantees termination, but may be inefficient or stop prematurely.</p></li>
<li><p><strong>Tolerance on parameters</strong>: stop when the change in parameters is small:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">new_guess</span> <span class="o">-</span> <span class="n">guess</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol_x</span>
</pre></div>
</div>
</li>
<li><p><strong>Tolerance on loss value</strong>: stop when the change in loss is small:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">abs</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">new_guess</span><span class="p">)</span> <span class="o">-</span> <span class="n">g</span><span class="p">(</span><span class="n">guess</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">tol_y</span>
</pre></div>
</div>
</li>
</ul>
<p>In most cases, you will want to use a combination of these conditions to avoid infinite loops and to balance speed vs. accuracy, and choosing tolerances is problem-dependent:</p>
<ul class="simple">
<li><p>For ill-scaled problems, small changes in <span class="math notranslate nohighlight">\(\vec{\lambda}\)</span> might yield large changes in loss.</p></li>
<li><p>For flat landscapes (e.g., vanishing gradients), changes in loss may be negligible for many steps.</p></li>
</ul>
<p>It’s also common to set a <strong>maximum number of iterations</strong> as a fallback, even if using convergence-based criteria.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In machine learning contexts, a full loop over the dataset is often called an <strong>epoch</strong>. If batch gradient descent is used, then the number of epochs is equivalent to the number of iterations. In stochastic or minibatch gradient descent, one epoch may involve many updates. Regardless of terminology, the same types of stopping criteria (loss tolerance, parameter change, or number of passes) apply.</p>
</div>
<p>There are many improved versions of gradient descent used in practice, especially in machine learning. These include:</p>
<ul class="simple">
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: only a random subset of the data is used at each step, which reduces computational cost and introduces randomness that can help escape local minima.</p></li>
<li><p><strong>Momentum-based methods</strong>: such as classical momentum or Nesterov Accelerated Gradient (NAG), which incorporate information about previous steps to accelerate learning.</p></li>
<li><p><strong>Adam optimizer</strong>: combines momentum and adaptive learning rates; it is the default in many deep learning libraries.</p></li>
</ul>
<p>Although we will not use these in this course, it is important to know that basic gradient descent forms the foundation of all these advanced methods.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The behavior of gradient descent is highly sensitive to:</p>
<ul class="simple">
<li><p>The initial guess: poor starting values may lead to slow convergence or incorrect minima.</p></li>
<li><p>The number of steps: too few steps may not allow convergence.</p></li>
<li><p>The step size (<code class="docutils literal notranslate"><span class="pre">h</span></code>): too large can overshoot or diverge; too small may converge too slowly.</p></li>
</ul>
<p>For most real problems, it’s best to use a tested optimization library rather than tuning these manually.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Try changing the step size <code class="docutils literal notranslate"><span class="pre">h</span></code> in the gradient descent loop to different values like <code class="docutils literal notranslate"><span class="pre">0.01</span></code>, <code class="docutils literal notranslate"><span class="pre">0.5</span></code>, and <code class="docutils literal notranslate"><span class="pre">1.0</span></code>. How does this affect convergence? Also try initializing <code class="docutils literal notranslate"><span class="pre">guess</span></code> with very bad values. What happens to the final loss?</p>
</div>
</section>
</section>
<section id="optimization-with-scipy">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Optimization with Scipy</a><a class="headerlink" href="#optimization-with-scipy" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">scipy</span></code> package is another commonly-used package that comes with lots of algorithms. In particular, there are a number of numerical optimization algorithms available through the <code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize</span></code> function. In practice, we will typically rely on <code class="docutils literal notranslate"><span class="pre">scipy.minimize</span></code> to handle minimization rather than writing our own algorithms. These algorithms will be more efficient, and have built-in techniques for estimating derivatives (or manage to optimize without derivatives at all).</p>
<p>One of the most commonly-used algorithms is the <a class="reference external" href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">“BFGS” algorithm</a>, named after its creators Broyden, Fletcher, Goldfarb, and Shanno. When in doubt, this is a good algorithm to try first. Let’s see how it works for our problem:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w">  </span><span class="nn">scipy.optimize</span><span class="w">  </span><span class="kn">import</span> <span class="n">minimize</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">bad_guess</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
<span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: 4.535847841346485e-13
        x: [ 7.000e-01  3.000e-01  5.000e-01  2.000e-01  1.000e-01
             5.000e-01]
      nit: 28
      jac: [-1.508e-07  2.820e-08 -8.943e-07 -3.180e-07 -6.586e-07
             7.751e-08]
 hess_inv: [[ 1.086e+01 -2.311e+00 ... -1.207e-01  1.951e-01]
            [-2.311e+00  3.484e+00 ... -5.994e-01 -2.638e+00]
            ...
            [-1.207e-01 -5.994e-01 ...  3.661e-01  2.177e-02]
            [ 1.951e-01 -2.638e+00 ...  2.177e-02  9.058e+00]]
     nfev: 217
     njev: 31
</pre></div>
</div>
</div>
</div>
<p>A few things to note here:</p>
<ul class="simple">
<li><p>We still had to use the function <code class="docutils literal notranslate"><span class="pre">g</span></code> that only takes a single argument (the variable we want to optimize with respect to).</p></li>
<li><p>The output is not a single number, but rather a Python class with various attributes.</p></li>
<li><p>This was really fast!</p></li>
</ul>
<p>We can investigate the attributes of the output using the <code class="docutils literal notranslate"><span class="pre">dir</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">dir</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;fun&#39;,
 &#39;hess_inv&#39;,
 &#39;jac&#39;,
 &#39;message&#39;,
 &#39;nfev&#39;,
 &#39;nit&#39;,
 &#39;njev&#39;,
 &#39;status&#39;,
 &#39;success&#39;,
 &#39;x&#39;]
</pre></div>
</div>
</div>
</div>
<p>Now we can check various aspects of the result using the <code class="docutils literal notranslate"><span class="pre">.</span></code> operator:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">success</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.69999993, 0.29999964, 0.49999979, 0.19999771, 0.1000001 ,
       0.50000067])
</pre></div>
</div>
</div>
</div>
<p>This tells us that the optimization was successful, and gives us the final result. Let’s compare this to the original input:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Actual Input: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">x</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Regression Result: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">lamda</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Actual Input: [0.69999993 0.29999964 0.49999979 0.19999771 0.1000001  0.50000067]
Regression Result: [0.3, 0.7, 0.2, 0.5, 0.5, 0.1]
</pre></div>
</div>
</div>
</div>
<p>We can see that this was much faster than our naive gradient descent, and is also more accurate (although the order of the peaks is still switched due to the bad initial guess).</p>
<p>Let’s revisit the real spectra we worked with earlier and try to optimize the peak positions and widths:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b818a405c90daed6d93a8827a39f70252b01fd4e6eaa1a142ad781c3404c4c6d.png" src="../_images/b818a405c90daed6d93a8827a39f70252b01fd4e6eaa1a142ad781c3404c4c6d.png" />
</div>
</div>
<p>Remember that we used guesses of the peak position and width:</p>
<p><span class="math notranslate nohighlight">\(y_i = w_0 \exp\left(-\frac{(x_i-2900)^2}{2(25^2)}\right) + w_1 \exp\left(-\frac{(x_i-2980)^2}{2(25^2)}\right)\)</span></p>
<p>Then we optimized the parameters, <span class="math notranslate nohighlight">\(\vec{w}\)</span>, and found <span class="math notranslate nohighlight">\(w_0 = 0.545\)</span> and <span class="math notranslate nohighlight">\(w_1 = 0.675\)</span>. We can convert these parameters into the <span class="math notranslate nohighlight">\(\vec{\lambda}\)</span> format and use our <code class="docutils literal notranslate"><span class="pre">two_gaussians</span></code> function to check the initial guess:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">guess</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.545</span><span class="p">,</span> <span class="mf">0.675</span><span class="p">,</span> <span class="mi">2900</span><span class="p">,</span> <span class="mi">2980</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="n">y_guess</span> <span class="o">=</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">guess</span><span class="p">,</span> <span class="n">x_peak</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_guess</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/46074cf6070db607332fd4e6e9027b6dfcc64848b68975b2bc6354e682245fc4.png" src="../_images/46074cf6070db607332fd4e6e9027b6dfcc64848b68975b2bc6354e682245fc4.png" />
</div>
</div>
<p>Visualizing the initial guess is a good idea whenever possible (e.g. when fitting spectra), since it ensures that you are starting from a resonable point. We can use the same loss function as before to optimize the other parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">g</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_peak</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gaussian_loss</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">guess</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
<span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: 0.0003472560732986476
        x: [ 4.763e-01  5.649e-01  2.905e+03  2.985e+03  3.882e+01
             2.539e+01]
      nit: 79
      jac: [ 3.424e-06 -2.041e-07 -7.025e-09 -9.839e-08  1.010e-07
             5.166e-09]
 hess_inv: [[ 7.061e+00  1.320e+00 ... -2.407e+02 -1.888e+02]
            [ 1.320e+00  2.625e+01 ... -2.123e+03  5.054e+02]
            ...
            [-2.407e+02 -2.123e+03 ...  2.488e+05 -7.838e+04]
            [-1.888e+02  5.054e+02 ... -7.838e+04  5.666e+04]]
     nfev: 588
     njev: 84
</pre></div>
</div>
</div>
</div>
<p>It looks successful! Let’s see how well it worked:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fitted</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
<span class="n">y_fitted</span> <span class="o">=</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">fitted</span><span class="p">,</span> <span class="n">x_peak</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_fitted</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/cf5c588a7f1ec94a84b4c2d534242fc3e35667cc6e0d77a08ad840e2362b843e.png" src="../_images/cf5c588a7f1ec94a84b4c2d534242fc3e35667cc6e0d77a08ad840e2362b843e.png" />
</div>
</div>
<p>This looks much better!</p>
<p>We can also add constraints to the loss function. For example, we might expect that the peak width (standard deviation) should be similar for both peaks. We can enforce this by adding an additional term to the loss function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">g_simwidth</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_peak</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gaussian_loss</span><span class="p">(</span><span class="n">lamda</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">lamda</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamda</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">g_simwidth</span><span class="p">,</span> <span class="n">guess</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
<span class="n">y_fitted</span> <span class="o">=</span> <span class="n">two_gaussians</span><span class="p">(</span><span class="n">fitted</span><span class="p">,</span> <span class="n">x_peak</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_peak</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_peak</span><span class="p">,</span> <span class="n">y_fitted</span><span class="p">);</span>
<span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  message: Desired error not necessarily achieved due to precision loss.
  success: False
   status: 2
      fun: 0.0016853103771842987
        x: [ 5.450e-01  6.750e-01  2.900e+03  2.980e+03  2.500e+01
             2.500e+01]
      nit: 0
      jac: [-9.752e-05 -7.127e-05 -3.202e-05  5.509e-05  9.997e-01
             9.997e-01]
 hess_inv: [[1 0 ... 0 0]
            [0 1 ... 0 0]
            ...
            [0 0 ... 1 0]
            [0 0 ... 0 1]]
     nfev: 194
     njev: 26
</pre></div>
</div>
<img alt="../_images/46074cf6070db607332fd4e6e9027b6dfcc64848b68975b2bc6354e682245fc4.png" src="../_images/46074cf6070db607332fd4e6e9027b6dfcc64848b68975b2bc6354e682245fc4.png" />
</div>
</div>
<p>We can see that the fit quality is similar, but now the peak widths are nearly identical. However, they are not exactly the same, since the loss function constraint is “soft” – the peak widths will deviate if it makes the fit much better.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sometimes we want to enforce that all parameters meet physical constraints — for example, that all weights <span class="math notranslate nohighlight">\(w_i\)</span> are positive.</p>
<p>One common approach is to modify the loss function to penalize violations of the constraint. For instance, you could add a differentiable penalty term like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penalty</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">w</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">lamda</span><span class="p">[:</span><span class="n">m</span><span class="p">]]))</span>
</pre></div>
</div>
<p>to the loss function. This discourages negative weights but does not absolutely prevent them. These are known as <strong>soft constraints</strong>.</p>
<p>Alternatively, many optimizers (including <code class="docutils literal notranslate"><span class="pre">scipy.minimize</span></code>) support <strong>bounds</strong> or <strong>constraints</strong> that can enforce this behavior exactly. These are called <strong>hard constraints</strong>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Exercise</p>
<p>Modify the Gaussian loss function to include a soft constraint that penalizes negative weights using a root-mean-square penalty. What happens to the fit when the initial guess includes a negative weight? Try different values of the penalty term’s weight to control the strength of the constraint.</p>
</div>
</section>
<section id="additional-reading">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Additional Reading</a><a class="headerlink" href="#additional-reading" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">SciPy Optimize Documentation</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic Differentiation – Wikipedia</a></p></li>
<li><p><a class="reference external" href="https://link.springer.com/book/10.1007/978-0-387-40065-5">Nocedal and Wright, <em>Numerical Optimization</em></a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6980">Understanding the Adam Optimizer</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./1-numerical_methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Topic1.3-Linear_Regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Linear Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="../2-regression/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-regression">Non-linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-differentiation">Automatic Differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stopping-criteria">Stopping Criteria</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-scipy">Optimization with Scipy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-reading">Additional Reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By A.J. Medford
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  © 2025 A.J. Medford
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>